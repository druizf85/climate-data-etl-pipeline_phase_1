# Temperatura Ambiente del Aire ¬∑ Fase 1 ‚Äî ETL Pipeline

> **Objetivo de la Fase 1**: construir un pipeline de datos **confiable, reproducible y eficiente** para integrar el dataset p√∫blico *Temperatura Ambiente del Aire* (IDEAM, Colombia), consolidar el hist√≥rico **2005‚Äì2024** y habilitar una ingesta **diaria** desde **2025** en adelante. El resultado es una tabla √∫nica en PostgreSQL (`hist_temp`) lista para an√°lisis y dashboards.

---

## üìå Resumen del proyecto

* **Fuente**: API de Datos Abiertos de Colombia (IDEAM) ‚Äî *Temperatura Ambiente del Aire*.
* **Periodo**: 2005-01-01 a la fecha.
* **Formato de respaldo**: Parquet (capas **raw** y **clean**).
* **Base de datos destino**: PostgreSQL.
* **Carga**: Bulk load v√≠a `COPY` ‚Üí *staging* temporal ‚Üí *merge* a `hist_temp` con `ON CONFLICT DO NOTHING` para controlar duplicidad en la informaci√≥n.
* **Orquestaci√≥n**: Airflow (DAG diario para ingesta incremental).
* **Trazabilidad**: batches con `batch` y backups por cortes de extracci√≥n.

---

## üóÇÔ∏è Estructura del repositorio

```
project/
‚îú‚îÄ‚îÄ‚îÄdata
‚îÇ   ‚îú‚îÄ‚îÄ‚îÄclean_data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄhist_data
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄnew_data
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄraw_data
‚îÇ       ‚îú‚îÄ‚îÄ‚îÄhist_data
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄnew_data
‚îú‚îÄ‚îÄ‚îÄmodels
‚îú‚îÄ‚îÄ‚îÄqueries_src
‚îÇ   ‚îú‚îÄ‚îÄ‚îÄqueries_hist_extraction
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄqueries_new_extraction
‚îú‚îÄ‚îÄ‚îÄsrc (paquetes de extracci√≥n, transformaci√≥n y carga)

```

**Convenciones**

* `raw_*`: datos tal cual se extraen de la API (preprocesados, respaldo).
* `clean_*`: datos limpios y listos para carga (revisi√≥n de datos duplicados, revisi√≥n de datos nulos, inclusi√≥n de informaci√≥n, tipificaci√≥n y derivaciones).
* `hist_*`: cortes hist√≥ricos (2005‚Äì2024).
* `new_*`: ingestas diarias (2025‚Äìpresente).

---

## üîó Fuente de datos

**Dataset**: *Temperatura Ambiente del Aire* ‚Äî IDEAM (Datos Abiertos de Colombia).

* Medici√≥n **horaria** por estaci√≥n y sensor en el territorio colombiano.
* Verificados bajo control de calidad b√°sico (OMM).

> El proyecto conserva los archivos en Parquet y registra los cortes de extracci√≥n, manteniendo **respaldo** y **reproducibilidad**.

---

## üß± Esquema de la tabla destino (`hist_temp`)

```sql
CREATE TABLE IF NOT EXISTS hist_temp (
  identificador      VARCHAR(100) PRIMARY KEY, -- codigoestacion || '-' || codigosensor || '-' || fechaobservacion
  batch              VARCHAR(100) NOT NULL,    -- etiqueta del corte/ingesta
  codigoestacion     VARCHAR(100) NOT NULL,
  codigosensor       VARCHAR(100) NOT NULL,
  fechaobservacion   TIMESTAMP    NOT NULL,
  mesobservacion     INT          NOT NULL,
  valorobservado     FLOAT        NOT NULL,
  nombreestacion     VARCHAR(100) NOT NULL,
  departamento       VARCHAR(100) NOT NULL,
  municipio          VARCHAR(100) NOT NULL,
  zonahidrografica   VARCHAR(100) NOT NULL,
  latitud            FLOAT        NOT NULL,
  longitud           FLOAT        NOT NULL,
  descripcionsensor  VARCHAR(100) NOT NULL,
  unidadmedida       VARCHAR(100) NOT NULL
);
```

**identificador**: `identificador = codigoestacion + codigosensor + fechaobservacion`. Se controlan las inserciones con `ON CONFLICT (identificador) DO NOTHING`.

---

## ‚öôÔ∏è Pipeline (Fase 1)

### 1) Extracci√≥n

* Hist√≥rico **2005‚Äì2024** (ejecuci√≥n √∫nica) y extracci√≥n **incremental diaria** desde **2025-01-01**.
* Preprocesamiento m√≠nimo:

  * Conversi√≥n de `fechaobservacion` a `TIMESTAMP`.
  * Conversi√≥n a `FLOAT` en `valorobservado`, `latitud`, `longitud`.
* Almacenamiento de backups en formato Parquet: `data/raw_data/{hist_data|new_data}`.

### 2) Transformaci√≥n

* **Limpieza**: eliminaci√≥n de duplicados y nulos.
* **Estandarizaci√≥n**: renombrado y tipificaci√≥n de columnas.
* **Derivadas**: `mesobservacion`, `batch` (fecha/corte), `identificador` (PK).
* Almacenamiento de backups en formato Parquet: `data/clean_data/{hist_data|new_data}`.

### 3) Carga (PostgreSQL)

1. Exportar el DataFrame limpio a **CSV temporal**.
2. Crear **tabla temporal** (`tmp_hist_temp`) con la misma estructura que `hist_temp`.
3. Cargar CSV ‚Üí `tmp_hist_temp` con **`COPY`** .
4. Insertar de `tmp_hist_temp` ‚Üí `hist_temp` con **`ON CONFLICT (identificador) DO NOTHING`**.
5. Limpiar: `DROP` de la temporal, eliminar CSV temporal y cerrar conexiones.

---

## üìà Volumen hist√≥rico (resumen de cortes cargados)

1. 2005-01-01 00:00:00 ‚Üí 2008-12-31 23:00:00 ‚Äî **2,494,861** registros
2. 2009-01-01 00:00:00 ‚Üí 2013-12-31 23:45:00 ‚Äî **7,407,674**
3. 2014-01-01 00:00:00 ‚Üí 2015-12-31 23:45:00 ‚Äî **3,971,491**
4. 2016-01-01 00:00:00 ‚Üí 2017-12-31 23:45:00 ‚Äî **11,317,877**
5. 2018-01-01 00:00:00 ‚Üí 2018-12-31 23:45:00 ‚Äî **8,671,255**
6. 2019-01-01 00:00:00 ‚Üí 2019-12-31 23:45:00 ‚Äî **13,205,603**
7. 2020-01-01 00:00:00 ‚Üí 2020-12-31 23:45:00 ‚Äî **9,632,775**
8. 2021-01-01 00:00:00 ‚Üí 2021-12-31 23:45:00 ‚Äî **6,324,941**
9. 2022-01-01 00:00:00 ‚Üí 2023-12-31 23:44:00 ‚Äî **9,888,260**
10. 2024-01-01 00:00:00 ‚Üí 2024-12-31 23:44:00 ‚Äî **6,443,780**

---

## üß™ Calidad de datos y controles

* **PK** y **UNIQUE** impl√≠cito en `identificador` ‚Üí evita duplicados en destino.
* **Null handling** en transformaci√≥n (rechazo o imputaci√≥n seg√∫n regla).
* **Tipificaci√≥n**: control de fechas formato est√°ndar, unificaci√≥n de datos num√©ricos.
* **Logs/estad√≠sticas** por batch (filas en raw, filas insertadas, control de tiempos en extracci√≥n y en ejecuci√≥n del dag).

---

## üß≠ Orquestaci√≥n (Airflow)

* DAG diario: extrae **desde el √∫ltimo `batch`** hasta `now()-1 day`.
* Tareas: `extract_function` ‚Üí `transform_function` ‚Üí `load_function` 
* Reintentos, alertas y m√©tricas de duraci√≥n por tarea.

---

## üöÄ Ejecuci√≥n

1. **Hist√≥rico**: ejecutar el flujo `hist` (una sola vez) hasta poblar `hist_temp`.
2. **Diario**: habilitar DAG incremental (agenda diaria; ventana desde el √∫ltimo `batch`).

---

## üîß Tecnolog√≠a

* **Python** (extracci√≥n, transformaci√≥n, utilidades).
* **DuckDB** (Parquet ‚Üí Conversi√≥n a CSV para inserci√≥n de datos).
* **PostgreSQL** (tabla `hist_temp`, bulk load con `COPY`).
* **SQLAlchemy/psycopg2** (conexi√≥n y `raw_connection`).
* **Airflow** (orquestaci√≥n de hist√≥rico e incremental).

---

## ‚ùó Limitaciones y notas a considerar

Se encontraron algunas limitantes en el proceso:

* Dependencia de disponibilidad y l√≠mites de la API de origen (1000 registros por consulta) periodos de mantenimiento de la plataforma, entre otros.
* Lecturas horarias: huecos posibles por ca√≠da de estaciones o API.


---

## üó∫Ô∏è Pr√≥ximos pasos (Fase 2 y 3 ‚Äî fuera de alcance de esta fase)

* **Fase 2 (dbt)**: tests (unique, not null, valores permitidos, transformaciones y vistas personalizadas), documentaci√≥n.
* **Fase 3 (BI)**: dashboards en Power BI (tendencias, estacionalidad, mapas, anomal√≠as) y KPIs por zona/estaci√≥n.
